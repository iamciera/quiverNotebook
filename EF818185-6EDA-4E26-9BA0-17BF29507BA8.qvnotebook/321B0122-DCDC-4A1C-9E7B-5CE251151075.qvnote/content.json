{
  "title": "Erin’s Talk April 6",
  "cells": [
    {
      "type": "markdown",
      "data": "# Why H20?\n\nThe software is to produce open source machine learning software that works on Hadoop and Spark. Written in Java, but the focus now is to use for R and Python. H20 is a distributed machine learning software. \n\nAPIs are available for R, Python, Scala & REST JSON (JSON API)\n\nIF you have big data you shouldnt have to sample it. In the beginning people would sample and this is not how the algorithm's were designed for. \n\nAlgorithms are not nessisaryily new, but there are more efficient ways to implement them. *Do they end up renaming?*\n\nDeep Neural Networks: Multi-layer Feed-Forward Neural Network takes in dataframes. The image based neural networks do not run on CPUs so are not implemented in H20 yet. \n\nYou cannot have missing values for PCA.\n\nH2o cluster: Java program that is running on your machine or remotley. Maybe install on server.\n\nDistributed Key Value Stor: Accounting system for keeping track of everything. How the whole thing works\n\nH2o: Distributed data frame. \n\nAmazon EC2 is used a lot. \n\n## Installization\n\nAll you need is R and and Java. Download [from website](http://h2o.ai/download). \n-   no computation is ever performed in R\n-   All coputations are performed (in highly optimized Java code) in the h2o cluster and intitiated by REST aclls from R\n\n## Code\n\nAll functions \n\nx = list of colums names\ny = name of response column\n\nThere are a bunch of parameters \n\nGrid search: Since there are a bunch of parameters, you can use grid search to find best model.  Different archetectures of the neural networks. Set the regulation terms. `h20.grid()`.  The grid is model specific. By default it will try all the combinations of that. There is also an option for random grid search. \n\n*What is the difference between a server and cluster?*\n\nDo we have a multinode server? There are scripts that they have that specify's how many nodes are used. I believe a node is a group of clusters??? \n\n## Python\n\n[scikit-learn](]http://scikit-learn.org/stable/) - good but not able to run on multipe nodes. But h2o sets up their python package to be simialr to scikit, but with the capablility to scale. \n\n## Inspect model performace \n\n`print(model)`- first pops up a training data. You can write your own performace metric, but h2o has the most popular.  AUC is caluclated off the raw predicted value.  If you need to Draw a line in a sand, you need to specify the threshold.  The threshold that maximizes the f score.  This all calculated and shows the confusion matrix. What was right and what was wrong. \n\n## Saving\n\nYOu can just do save.model().  Which saves in a binary format. What is the model exactly? If you save it, what are you saving exactly?\n\n## H2o Ensemble\n\nUses machine learning to to train, stacking a bunch of algorithms to get the best. So if you run these and end up with a model.\n\n**Meta learning algorithm** is the algorithm that binds them together. \n\nWhat you do is use cross validation of the base algorithms and that produces n=predicted value.  So each training during cross validation produces a predict value then you squish on the predictive value (new columns) and they become the new training data. Since they are all related you need meta learners to basically unconfound. Basically the output of all the models, which are wieghted to run yet another algorithms. What is the last learning algorithm to be run. \n\nWhy would you want to use this?  To get the best algorithm ever.  It takes more time and adds complexity. Very difficult to interpret model. You can looks at each seperatley. Maybe one model predicts better than the rest?\n\n[drat repository](https://github.com/eddelbuettel/drat)\n\n**cross validation**: cross-validation combines (averages) measures of fit (prediction error) to correct for the optimistic nature of training error and derive a more accurate estimate of model prediction performance.\n\nCross validation is used when you use the entire data set as a the training set? So cross validation see if the training set is truly represenative of the data by further samples and scoring how valid the training set is at representing the full set(?) In **k-fold cross validation**,the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged (or otherwise combined) to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used,[7] but in general k remains an unfixed parameter.\n\nWhen k=n (the number of observations), the k-fold cross-validation is exactly the leave-one-out cross-validation.\n\nIn stratified k-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels.\n\n## Hyperparameter optimization\n\nGrid Search / Parameter Sweep: \nRandom search: Not as computationally heavy\n\n## To-do \n\n- [ ] You really need to specify what your tasks are!\n- [ ] Is there a function in r for specifying your distribution?\n- [ ] learn more aout performace metric\n- [ ]  rpy2 python package\n\nIt is best to use three data sets.  How were these chosen?\n\n1. test set.  Only use once.\n2. validation set. Only need if testing multiple models or for like a grid search. \n3. test set. \n\n\n\n\n*Why would the scientific advisors help? Do they get paid?*\n\n"
    },
    {
      "type": "text",
      "data": "<br>"
    }
  ]
}